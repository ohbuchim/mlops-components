{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Step Functions Data Science SDK と Amazon SageMaker でデータ準備、AutoGluon を使ったモデル学習、バッチ推論パイプラインを構築"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このノートブックは、00-prepare-container-images.ipynb を実行してから実行してください。\n",
    "\n",
    "\n",
    "1. [背景](#背景)\n",
    "1. [セットアップ](#セットアップ)\n",
    "1. [S3 バケットの準備](#S3-バケットの準備)\n",
    "1. [データ](#データ)\n",
    "1. [ステータス通知用 SNS Topic の準備](#ステータス通知用-SNS-Topic-の準備)\n",
    "1. [ステータス通知用 Lambda 関数の準備](#ステータス通知用-Lambda-関数の準備)\n",
    "1. [AWS Step Functions の準備](#AWS-Step-Functions-の準備)\n",
    "1. [AWS Step Functions Workflow の実行](#AWS-Step-Functions-Workflow-の実行)\n",
    "1. [S3 へのファイル作成を AWS Step Functions Workflow 実行用 Lambda 関数のトリガーに設定](#S3-へのファイル作成を-AWS-Step-Functions-Workflow-実行用-Lambda-関数のトリガーに設定)\n",
    "1. [動作確認](#動作確認)\n",
    "1. [リソースの削除](#リソースの削除)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 背景\n",
    "\n",
    "AWS Step Functions は、機械学習パイプラインの構築でよく使われます。AWS Step Functions Data Science SDK を使うと、Python でパイプラインを作ることができるため、データサイエンティストが自身のユースケースに最適な構成を簡単に構築できます。Step Functions を使った基本的なパイプライン構築方法については [こちらのサンプルノートブック](https://github.com/aws-samples/aws-ml-jp/blob/main/mlops/step-functions-data-science-sdk/model-train-evaluate-compare/step_functions_mlworkflow_scikit_learn_data_processing_and_model_evaluation_with_experiments.ipynb) をご参照ください。 \n",
    "\n",
    "本ノートブックは、**データ準備、モデル学習、バッチ推論のパイプライン** を構築するためのサンプルノートブックです。扱うのはテーブル形式のデータで、モデルの学習には [AutoGluon-Tabular](https://auto.gluon.ai/stable/tutorials/tabular_prediction/index.html) を使用します。[こちらの記事](https://aws.amazon.com/jp/builders-flash/202201/autogluon-tabular-tutorials/?awsf.filter-name=*all) で日本語での簡単な使用方法が説明されています。サンプルデータとしては、公開されているカリフォルニア州の住宅データセットを使用します。ターゲット変数は、カリフォルニア州の地区の住宅価格の中央値です。\n",
    "\n",
    "<img src=\"workflow.png\" width=\"50%\">\n",
    "\n",
    "---\n",
    "\n",
    "## セットアップ\n",
    "\n",
    "このサンプルノートブックは長いので、実行したいセルにアクセスしやすいよう Table of Contents を作成する拡張機能をインストールすると便利です。以下のセルを実行したあと、このノートブックを開いているブラウザのタブをリロードすると Table of Contents の拡張機能が使えるようになります。以下のセルは、ノートブックインスタンスを再起動するまで再実行の必要はありません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install jupyter_contrib_nbextensions\n",
    "jupyter contrib nbextension install --user\n",
    "jupyter nbextension enable toc2/main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Functions Data Science SDK をインストール\n",
    "\n",
    "以下のセルを実行したら、**メニューの「Kernel」->「Restart」をクリックしてカーネルを再起動してください。**再起動後は以下のセルを再度実行する必要はないので、その下から作業を再開してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pip install -U awscli boto3 \"sagemaker>=2.0.0\"\n",
    "pip install -U \"stepfunctions==2.3.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "SageMaker セッションを作成し、設定を開始します。\n",
    "\n",
    "- 学習およびモデルデータに使用する S3 バケットとプレフィックスは、ノートブックインスタンス、トレーニング、およびホスティングと同じリージョン内にある必要があります。\n",
    "- データへの学習およびホスティングアクセスを提供するために使用される IAM ロール arn を用います。 ノートブックインスタンス、学習インスタンス、および/またはホスティングインスタンスに複数のロールが必要な場合は、 `sagemaker.get_execution_role（）` を、適切な IAM ロール arn 文字列に置き換えてください。\n",
    "\n",
    "**以下のセルの初めの方にある `mail_address` にご自身のメールアドレスを記載してから以下のセルを実行してください。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker.processing import Processor, ProcessingInput, ProcessingOutput\n",
    "from time import sleep\n",
    "\n",
    "%store -r\n",
    "\n",
    "mail_address = \"<SET YOUR E-MAIL ADDRESS>\"\n",
    "\n",
    "JST = tz.gettz('Asia/Tokyo')\n",
    "timestamp = datetime.now(JST).strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "iam_client = boto3.client('iam', region_name=region)\n",
    "sns_client = boto3.client('sns', region_name=region)\n",
    "ecr_client = boto3.client('ecr', region_name=region)\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "sagemaker_policy_name = project_name + '-' + user_name + '-policy'\n",
    "prefix = f'sagemaker/{project_name}/{user_name}'\n",
    "bucket_name = project_name + '-' + user_name + '-' + timestamp\n",
    "bucket_name_trigger = project_name + '-' + user_name + '-trigger-' + timestamp\n",
    "\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "lambda_client = boto3.client('lambda', region_name=region)\n",
    "\n",
    "policy_arn_list = []\n",
    "role_name_list = []\n",
    "lambda_function_list = []\n",
    "\n",
    "role_name = role.split('/')[-1]\n",
    "iam_console_url = f'https://{region}.console.aws.amazon.com/iamv2/home#/roles/details/{role_name}?section=permissions'\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "text = f\"\"\"\n",
    "以下の手順で IAM 関連の設定を実施してください。\n",
    "1. <a href=\\\"policy/sagemaker-policy.json\\\" target=\\\"_blank\\\">policy/sagemaker-policy.json</a> の中身をコピー\n",
    "1. <a href=\\\"https://{region}.console.aws.amazon.com/iam/home#/policies$new?step=edit\\\" target=\\\"_blank\\\">IAM Policy の作成</a>をクリックし、**JSON** タブをクリックしてから手順1でコピーした JSON をペーストして右下の **次のステップ：タグ** ボタンをクリック\n",
    "1. 右下の **次のステップ：確認** ボタンをクリック\n",
    "1. **名前** に **「{sagemaker_policy_name}」** を記載して、右下の **ポリシーの作成** ボタンをクリック\n",
    "1.  <a href=\\\"{iam_console_url}\\\" target=\\\"_blank\\\">ノートブックインスタンスにアタッチされた IAM Role</a> を開く\n",
    "1. **許可を追加** ボタンをクリックして **ポリシーをアタッチ** を選択\n",
    "1. **その他の許可ポリシー** の検索ボックスで手順4 で作成した {sagemaker_policy_name} を検索して横にあるチェックボックスをオンにする\n",
    "1. **ポリシーのアタッチ** をクリック\n",
    "\"\"\"\n",
    "display(Markdown(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IAM Role や Policy 作成用のヘルパー関数を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "def get_policy_arn(policy_name):\n",
    "    marker = ''\n",
    "    while True:\n",
    "        if marker == '':\n",
    "            response = iam_client.list_policies(Scope='Local')\n",
    "        else:\n",
    "            response = iam_client.list_policies(Scope='Local', Marker=marker)\n",
    "        for content in response['Policies']:\n",
    "            if policy_name == content['PolicyName']:\n",
    "                return content['Arn']\n",
    "        if 'Marker' in response:\n",
    "            marker = response['Marker']\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return ''\n",
    "\n",
    "\n",
    "def detach_role_policies(role_name):\n",
    "    try:\n",
    "        response = iam_client.list_attached_role_policies(\n",
    "            RoleName=role_name,\n",
    "        )\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "    policies = response['AttachedPolicies']\n",
    "\n",
    "    for p in policies:\n",
    "        response = iam_client.detach_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyArn=p['PolicyArn']\n",
    "        )\n",
    "\n",
    "            \n",
    "def create_role(role_name, assume_role_policy):\n",
    "    try:\n",
    "        response = iam_client.create_role(\n",
    "            Path = '/service-role/',\n",
    "            RoleName = role_name,\n",
    "            AssumeRolePolicyDocument = json.dumps(assume_role_policy),\n",
    "            MaxSessionDuration=3600*12 # 12 hours\n",
    "        )\n",
    "        role_arn = response['Role']['Arn']\n",
    "    except Exception as ex:\n",
    "        if \"EntityAlreadyExists\" in str(ex):\n",
    "            detach_role_policies(role_name)\n",
    "            response = iam_client.delete_role(\n",
    "                RoleName = role_name,\n",
    "            )\n",
    "            response = iam_client.create_role(\n",
    "                Path = '/service-role/',\n",
    "                RoleName = role_name,\n",
    "                AssumeRolePolicyDocument = json.dumps(assume_role_policy),\n",
    "                MaxSessionDuration=3600*12 # 12 hours\n",
    "            )\n",
    "            role_arn = response['Role']['Arn']\n",
    "        else:\n",
    "            print(ex)\n",
    "    sleep(10)\n",
    "    return role_arn\n",
    "\n",
    "\n",
    "def create_policy(policy_name, policy_json_name):\n",
    "    with open('policy/' + policy_json_name, 'r') as f:\n",
    "        policy_json = json.load(f)\n",
    "    try:\n",
    "        response = iam_client.create_policy(\n",
    "            PolicyName=policy_name,\n",
    "            PolicyDocument=json.dumps(policy_json),\n",
    "        )\n",
    "        policy_arn = response['Policy']['Arn']\n",
    "    except Exception as ex:\n",
    "        if \"EntityAlreadyExists\" in str(ex):\n",
    "            response = iam_client.delete_policy(\n",
    "                PolicyArn=get_policy_arn(policy_name)\n",
    "            )\n",
    "            response = iam_client.create_policy(\n",
    "                PolicyName=policy_name,\n",
    "                PolicyDocument=json.dumps(policy_json),\n",
    "            )\n",
    "            policy_arn = response['Policy']['Arn']\n",
    "    policy_arn_list.append(policy_arn)\n",
    "    \n",
    "    sleep(10)\n",
    "    return policy_arn\n",
    "\n",
    "\n",
    "def create_policy_role(policy_name, policy_json_name, role_name, assume_role_policy):\n",
    "\n",
    "    role_arn = create_role(role_name, assume_role_policy)\n",
    "    policy_arn = create_policy(policy_name, policy_json_name)\n",
    "\n",
    "    sleep(5)\n",
    "    response = iam_client.attach_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyArn=policy_arn\n",
    "    )\n",
    "\n",
    "    role_name_list.append(role_name)\n",
    "    policy_arn_list.append(policy_arn)\n",
    "    sleep(10)\n",
    "    return role_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 バケットの準備\n",
    "### Job 生成物格納用 S3 バケットの準備\n",
    "\n",
    "SageMaker Jobs が生成したデータやモデルなどを保存する S3 バケットを作成します。セキュリティのため暗号化を有効にします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bucket(bucket_name):\n",
    "\n",
    "    if region == 'us-east-1':\n",
    "        response = s3_client.create_bucket(Bucket=bucket_name)\n",
    "    else:\n",
    "        location = {'LocationConstraint': region}\n",
    "        response = s3_client.create_bucket(Bucket=bucket_name,\n",
    "                                           CreateBucketConfiguration=location)\n",
    "    sleep(10)\n",
    "    response = s3_client.put_bucket_encryption(\n",
    "        Bucket=bucket_name,\n",
    "        ServerSideEncryptionConfiguration={\n",
    "            'Rules': [\n",
    "                {\n",
    "                    'ApplyServerSideEncryptionByDefault': {\n",
    "                        'SSEAlgorithm': 'AES256',\n",
    "                    },\n",
    "                },\n",
    "            ]\n",
    "        },\n",
    "    )\n",
    "\n",
    "    response = s3_client.put_public_access_block(\n",
    "        Bucket=bucket_name,\n",
    "        PublicAccessBlockConfiguration={\n",
    "            'BlockPublicAcls': True,\n",
    "            'IgnorePublicAcls': True,\n",
    "            'BlockPublicPolicy': True,\n",
    "            'RestrictPublicBuckets': True\n",
    "        },\n",
    "        ExpectedBucketOwner=account_id\n",
    "    )\n",
    "create_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トリガーとなるファイルアップロード用 S3 バケットの準備\n",
    "\n",
    "このサンプルノートブックでは、バッチ推論の入力ファイルをアップロードする S3 バケットを、SageMaker Jobs の出力保存用バケットとは別に用意します。同じバケットを利用してももちろんかまいません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bucket(bucket_name_trigger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ\n",
    "\n",
    "このサンプルノートブックで使用するデータセットは、1990 年の米国国勢調査の結果を使って作成された StatLib リポジトリ (http://lib.stat.cmu.edu/datasets/) から入手したもので、各行が国勢調査ブロックグループに対応しています。ブロックグループとは、米国国勢調査局がサンプルデータを公表している最小の地理的単位で、人口は通常 600～3,000人です。 \n",
    "\n",
    "### データの取得\n",
    "\n",
    "AWS が用意した S3 バケットからデータをダウンロードして展開します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://sagemaker-sample-files/datasets/tabular/california_housing/cal_housing.tgz .\n",
    "!tar -xvzf  cal_housing.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ダウンロードしたデータを DataFrame として読み込み、CSV 形式で保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"longitude\",\n",
    "    \"latitude\",\n",
    "    \"housingMedianAge\",\n",
    "    \"totalRooms\",\n",
    "    \"totalBedrooms\",\n",
    "    \"population\",\n",
    "    \"households\",\n",
    "    \"medianIncome\",\n",
    "    \"medianHouseValue\",\n",
    "]\n",
    "\n",
    "target = \"medianHouseValue\"\n",
    "\n",
    "cal_housing_df = pd.read_csv(\"CaliforniaHousing/cal_housing.data\", names=columns, header=None)\n",
    "rawdata_name = 'rawdata.csv'\n",
    "cal_housing_df.to_csv(rawdata_name, index=None)\n",
    "cal_housing_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データをS3にアップロードする\n",
    "ML パイプラインのトリガーとなるデータアップロード用の S3 バケットにデータをアップロードします。S3 バケットにトリガーの設定をしていないため、この時点ではトリガーは発行されません。データセットを S3 にアップロードするには、 `sagemaker.Session.upload_data` 関数を使用します。 戻り値として入力した S3 のロケーションは、後で学習ジョブを実行するときに使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_s3_path = sagemaker_session.upload_data(path=rawdata_name, bucket=bucket_name_trigger, key_prefix=prefix + \"/rawdata\")\n",
    "print('Raw data S3 path:', raw_data_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ステータス通知用 SNS Topic の準備\n",
    "\n",
    "ML パイプラインの中で、実行状況を通知するための SNS Topic を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns_notification_topic_name =  project_name + '-notification-' + user_name\n",
    "response = sns_client.create_topic(\n",
    "    Name=sns_notification_topic_name\n",
    ")\n",
    "sns_notification_topic_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このノートブックの初めのセットアップのところで設定したメールアドレスに通知が送信されるよう設定します。以下のセルを実行してから数分すると、設定したメールアドレスに「AWS Notification - Subscription Confirmation」というタイトルのメールが届きます。このメールの本文にある「Confirm subscription」のリンクをクリックして、SNS Topic のサブスクリプションを有効にします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns_notification_topic_arn = response['TopicArn']\n",
    "response = sns_client.subscribe(\n",
    "    TopicArn=sns_notification_topic_arn,\n",
    "    Protocol='email',\n",
    "    Endpoint = mail_address,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ステータス通知用 Lambda 関数の準備\n",
    "\n",
    "作成した SNS Topic にメッセージを送信する Lambda 関数を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def create_lambda_function(function_name, file_name, role_arn, handler_name,\n",
    "                            envs={}, py_version='python3.9'):\n",
    "\n",
    "    with open(file_name+'.zip', 'rb') as f:\n",
    "        zip_data = f.read()\n",
    "        \n",
    "    if function_exists(function_name):\n",
    "        \n",
    "        response = lambda_client.update_function_configuration(\n",
    "            FunctionName=function_name,\n",
    "            Environment={\n",
    "                'Variables': envs\n",
    "            },\n",
    "        )\n",
    "        sleep(10)\n",
    "        response = lambda_client.update_function_code(\n",
    "            FunctionName=function_name,\n",
    "            ZipFile=zip_data,\n",
    "            Publish=True,\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        response = lambda_client.create_function(\n",
    "            FunctionName=function_name,\n",
    "            Role=role_arn,\n",
    "            Handler=handler_name+'.lambda_handler',\n",
    "            Runtime=py_version,\n",
    "            Code={\n",
    "                'ZipFile':zip_data\n",
    "            },\n",
    "            Environment={\n",
    "                'Variables': envs\n",
    "            },\n",
    "            Timeout=60*5, # 5 minutes\n",
    "            MemorySize=128, # 128 MB\n",
    "            Publish=True,\n",
    "            PackageType='Zip',\n",
    "        )\n",
    "    lambda_function_list.append(function_name)\n",
    "    return response['FunctionArn']\n",
    "\n",
    "def function_exists(function_name):\n",
    "    try:\n",
    "        lambda_client.get_function(\n",
    "            FunctionName=function_name,\n",
    "        )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_notification_function_name  = project_name + '-notification-' + user_name\n",
    "lambda_notification_policy_name = lambda_notification_function_name + '-policy'\n",
    "lambda_notification_role_name = lambda_notification_function_name + '-role'\n",
    "lambda_notification_json_name = 'lambda-notification-policy.json'\n",
    "\n",
    "assume_role_policy = {\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [{\"Sid\": \"\",\"Effect\": \"Allow\",\"Principal\": {\"Service\":\"lambda.amazonaws.com\"},\"Action\": \"sts:AssumeRole\"}]\n",
    "}\n",
    "\n",
    "lambda_notification_role_arn = create_policy_role(lambda_notification_policy_name, lambda_notification_json_name,\n",
    "                   lambda_notification_role_name, assume_role_policy)\n",
    "sleep(10) # wait until IAM is created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のセルでは、Lambda 関数で使用するライブラリとソースコードを zip に固めています。Lmabda 関数を作成する際は、以下の処理を実行した環境と同じ Python のバージョンのランタイムを指定してください。2022年7月現在、conda_python3 カーネルの Python バージョンは 3.8 なので、Lambda 関数の Python バージョンも 3.8 を指定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prepare_lambda_resource(function_name, code_path):\n",
    "    !rm -rf $function_name\n",
    "    !rm {function_name}.zip\n",
    "    !mkdir $function_name\n",
    "    !pip install pyyaml -t $function_name\n",
    "    !cp {code_path}/index.py $function_name\n",
    "    !cd $function_name && zip -r ../{function_name}.zip .\n",
    "prepare_lambda_resource(lambda_notification_function_name, 'code/notification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda 関数を作成します。環境変数に、先ほど作成した SNS Topic の ARN を設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = {\n",
    "    'SNS_TOPIC_ARN': sns_notification_topic_arn\n",
    "}\n",
    "lambda_notification_function_arn = create_lambda_function(lambda_notification_function_name,\n",
    "                                                   lambda_notification_function_name,\n",
    "                                                   lambda_notification_role_arn,\n",
    "                                                   'index',\n",
    "                                                   envs,\n",
    "                                                   py_version='python3.8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Step Functions の準備\n",
    "\n",
    "Step Functions Data Science SDK を使って、冒頭に示した Step Functions Workflow を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stepfunctions\n",
    "from stepfunctions import steps\n",
    "from stepfunctions.inputs import ExecutionInput, StepInput\n",
    "from stepfunctions.steps import (\n",
    "    Chain,\n",
    "    ProcessingStep,\n",
    ")\n",
    "from stepfunctions.template import TrainingPipeline\n",
    "from stepfunctions.template.utils import replace_parameters_with_jsonpath\n",
    "from stepfunctions.workflow import Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IAM Role と Policy の作成\n",
    "\n",
    "Step Functions の Workflow にセットする IAM Role を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "step_functions_policy_name = project_name + '-stepfunctions-' + user_name + '-policy'\n",
    "step_functions_role_name = project_name + '-stepfunctions-' + user_name + '-role'\n",
    "step_functions_policy_json_name = 'stepfunctions-policy.json'\n",
    "\n",
    "assume_role_policy = {\n",
    "      \"Version\": \"2012-10-17\",\n",
    "      \"Statement\": [{\"Sid\": \"\",\"Effect\": \"Allow\",\"Principal\": {\"Service\":\"states.amazonaws.com\"},\"Action\": \"sts:AssumeRole\"}]\n",
    "    }\n",
    "\n",
    "workflow_execution_role = create_policy_role(step_functions_policy_name, step_functions_policy_json_name,\n",
    "                   step_functions_role_name, assume_role_policy)\n",
    "workflow_execution_role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Functions Workflow 実行時のパラメータの準備\n",
    "\n",
    "Step Functions Workflow 実行時に指定するパラメータのスキーマを定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_input = ExecutionInput(\n",
    "    schema={\n",
    "        \"TimeStamp\": str,\n",
    "        \"PrepJobName\": str,\n",
    "        \"PrepInput\": str,\n",
    "        \"PrepOutput\": str,\n",
    "        \"TrainJobName\": str,\n",
    "        \"TrainInput\": str,\n",
    "        \"TrainOutput\": str,\n",
    "        \"PredJobName\": str,\n",
    "        \"PredArgs\": str,\n",
    "        \"PredInput\": str,\n",
    "        \"PredOutput\": str,\n",
    "        \"PostJobName\": str,\n",
    "        \"PostInput\": str,\n",
    "        \"PostOutput\": str,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation Step の作成\n",
    "\n",
    "データ準備部分を作成します。まず SageMaker Processing 用の Processor を作成し、それを ProcessingStep のパラメタにセットします。このとき、Processing で使用するソースコードやコンテナイメージも指定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_segment = 2\n",
    "\n",
    "prep_timestamp = datetime.now(JST).strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "code_path = '/opt/ml/processing/input/code'\n",
    "input_dir = '/opt/ml/processing/input/data'\n",
    "output_dir = '/opt/ml/processing/output'\n",
    "\n",
    "SCRIPT_LOCATION = \"code/prep\"\n",
    "\n",
    "code_s3_path = sagemaker_session.upload_data(\n",
    "    SCRIPT_LOCATION,\n",
    "    bucket=bucket_name,\n",
    "    key_prefix=os.path.join(prefix, SCRIPT_LOCATION, prep_timestamp),\n",
    ")\n",
    "\n",
    "prep_inputs = [\n",
    "    ProcessingInput(\n",
    "        input_name='code',\n",
    "        source=code_s3_path,\n",
    "        destination=code_path),\n",
    "    ProcessingInput(\n",
    "        source=execution_input[\"PrepInput\"],\n",
    "        destination=input_dir,\n",
    "        input_name=\"data\"\n",
    "    )\n",
    "]\n",
    "\n",
    "prep_outputs = [\n",
    "    ProcessingOutput(\n",
    "        source=output_dir,\n",
    "        destination=execution_input[\"PrepOutput\"],\n",
    "        output_name=\"result\",\n",
    "    )\n",
    "]\n",
    "\n",
    "prep_processor = Processor(\n",
    "        role=role,\n",
    "        image_uri=prep_repository_uri,\n",
    "#         entrypoint=[\"python3\", f\"{code_path}/prep.py\"],\n",
    "        instance_count=1, \n",
    "        instance_type=\"ml.m5.xlarge\",\n",
    "        volume_size_in_gb=16,\n",
    "        volume_kms_key=None,\n",
    "        output_kms_key=None,\n",
    "        max_runtime_in_seconds=86400,  # default is 24 hours(60*60*24)\n",
    "        sagemaker_session=None,\n",
    "        env=None,\n",
    "        network_config=None\n",
    ")\n",
    "\n",
    "prep_step = ProcessingStep(\n",
    "    \"Data Preparation\",\n",
    "    processor=prep_processor,\n",
    "    job_name=execution_input[\"PrepJobName\"],\n",
    "    inputs=prep_inputs,\n",
    "    outputs=prep_outputs,\n",
    "    container_arguments=['--num-of-dataset', str(num_of_segment)],\n",
    "    container_entrypoint=[\"python3\", \"/opt/ml/processing/input/code/prep.py\"],\n",
    "    wait_for_completion=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training Step の作成\n",
    "\n",
    "データ準備の時と同様に、モデル学習用の ProcessingStep を作成します。データ準備との違いは、学習データ用の ProcessingInput に `s3_data_distribution_type='ShardedByS3Key'` が追加されていることです。これにより、指定された S3 パスにあるファイルが各インスタンスに均等に分配されます。ファイルの数と Processing Job で使用するインスタンス数を同じにすれば、1インスタンス1ファイルの状態を作ることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_timestamp = datetime.now(JST).strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "code_path = '/opt/ml/processing/input/code'\n",
    "input_dir = '/opt/ml/processing/input/data'\n",
    "output_dir = '/opt/ml/processing/output'\n",
    "\n",
    "SCRIPT_LOCATION = \"code/train\"\n",
    "\n",
    "code_s3_path = sagemaker_session.upload_data(\n",
    "    SCRIPT_LOCATION,\n",
    "    bucket=bucket_name,\n",
    "    key_prefix=os.path.join(prefix, SCRIPT_LOCATION, train_timestamp),\n",
    ")\n",
    "\n",
    "train_inputs = [\n",
    "    ProcessingInput(\n",
    "        input_name='code',\n",
    "        source=code_s3_path,\n",
    "        destination=code_path),\n",
    "    ProcessingInput(\n",
    "        source=execution_input[\"TrainInput\"],\n",
    "        destination=input_dir,\n",
    "        input_name=\"data\",\n",
    "        s3_data_distribution_type='ShardedByS3Key'\n",
    "    )\n",
    "]\n",
    "\n",
    "train_outputs = [\n",
    "    ProcessingOutput(\n",
    "        source=output_dir,\n",
    "        destination=execution_input[\"TrainOutput\"],\n",
    "        output_name=\"result\",\n",
    "    )\n",
    "]\n",
    "\n",
    "train_processor = Processor(\n",
    "        role=role,\n",
    "        image_uri=train_repository_uri,\n",
    "#         entrypoint=[\"python3\", f\"{code_path}/train.py\"],\n",
    "        instance_count=num_of_segment, \n",
    "        instance_type=\"ml.m5.xlarge\",\n",
    "        volume_size_in_gb=16,\n",
    "        volume_kms_key=None,\n",
    "        output_kms_key=None,\n",
    "        max_runtime_in_seconds=86400,  # default is 24 hours(60*60*24)\n",
    "        sagemaker_session=None,\n",
    "        env=None,\n",
    "        network_config=None\n",
    "    )\n",
    "\n",
    "train_step = ProcessingStep(\n",
    "    \"Model Training\",\n",
    "    processor=train_processor,\n",
    "    job_name=execution_input[\"TrainJobName\"],\n",
    "    inputs=train_inputs,\n",
    "    outputs=train_outputs,\n",
    "    container_arguments=['--num-of-dataset', str(num_of_segment)],\n",
    "    container_entrypoint=[\"python3\", \"/opt/ml/processing/input/code/train.py\"],\n",
    "    wait_for_completion=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### バッチ推論 Step の作成\n",
    "\n",
    "バッチ推論 Steo もモデル学習 Step と同様にファイルを各インスタンスに分散させて処理を行うため、ProcessingInput で `s3_data_distribution_type='ShardedByS3Key'` を使用しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_timestamp = datetime.now(JST).strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "code_path = '/opt/ml/processing/input/code'\n",
    "input_dir = '/opt/ml/processing/input/data'\n",
    "output_dir = '/opt/ml/processing/output'\n",
    "\n",
    "SCRIPT_LOCATION = \"code/pred\"\n",
    "\n",
    "code_s3_path = sagemaker_session.upload_data(\n",
    "    SCRIPT_LOCATION,\n",
    "    bucket=bucket_name,\n",
    "    key_prefix=os.path.join(prefix, SCRIPT_LOCATION, pred_timestamp),\n",
    ")\n",
    "\n",
    "pred_inputs = [\n",
    "    ProcessingInput(\n",
    "        input_name='code',\n",
    "        source=code_s3_path,\n",
    "        destination=code_path),\n",
    "    ProcessingInput(\n",
    "        source=execution_input[\"PredInput\"],\n",
    "        destination=input_dir,\n",
    "        input_name=\"data\",\n",
    "        s3_data_distribution_type='ShardedByS3Key'\n",
    "    )\n",
    "]\n",
    "\n",
    "pred_outputs = [\n",
    "    ProcessingOutput(\n",
    "        source=output_dir,\n",
    "        destination=execution_input[\"PredOutput\"],\n",
    "        output_name=\"result\",\n",
    "    )\n",
    "]\n",
    "\n",
    "pred_processor = Processor(\n",
    "        role=role,\n",
    "        image_uri=train_repository_uri,\n",
    "#         entrypoint=[\"python3\", f\"{code_path}/pred.py\"],\n",
    "        instance_count=num_of_segment, \n",
    "        instance_type=\"ml.m5.xlarge\",\n",
    "        volume_size_in_gb=16,\n",
    "        volume_kms_key=None,\n",
    "        output_kms_key=None,\n",
    "        max_runtime_in_seconds=86400,  # default is 24 hours(60*60*24)\n",
    "        sagemaker_session=None,\n",
    "        env=None,\n",
    "        network_config=None\n",
    "    )\n",
    "\n",
    "pred_step = ProcessingStep(\n",
    "    \"Batch Inference\",\n",
    "    processor=train_processor,\n",
    "    job_name=execution_input[\"PredJobName\"],\n",
    "    inputs=pred_inputs,\n",
    "    outputs=pred_outputs,\n",
    "    container_arguments=execution_input[\"PredArgs\"],\n",
    "    container_entrypoint=[\"python3\", \"/opt/ml/processing/input/code/pred.py\"],\n",
    "    wait_for_completion=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 後処理 Step の作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_timestamp = datetime.now(JST).strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "code_path = '/opt/ml/processing/input/code'\n",
    "input_dir = '/opt/ml/processing/input/data'\n",
    "output_dir = '/opt/ml/processing/output'\n",
    "\n",
    "SCRIPT_LOCATION = \"code/post\"\n",
    "\n",
    "code_s3_path = sagemaker_session.upload_data(\n",
    "    SCRIPT_LOCATION,\n",
    "    bucket=bucket_name,\n",
    "    key_prefix=os.path.join(prefix, SCRIPT_LOCATION, post_timestamp),\n",
    ")\n",
    "\n",
    "post_inputs = [\n",
    "    ProcessingInput(\n",
    "        input_name='code',\n",
    "        source=code_s3_path,\n",
    "        destination=code_path),\n",
    "    ProcessingInput(\n",
    "        source=execution_input[\"PostInput\"],\n",
    "        destination=input_dir,\n",
    "        input_name=\"data\"\n",
    "    )\n",
    "]\n",
    "\n",
    "post_outputs = [\n",
    "    ProcessingOutput(\n",
    "        source=output_dir,\n",
    "        destination=execution_input[\"PostOutput\"],\n",
    "        output_name=\"result\",\n",
    "    )\n",
    "]\n",
    "\n",
    "post_processor = Processor(\n",
    "        role=role,\n",
    "        image_uri=prep_repository_uri,\n",
    "#         entrypoint=[\"python3\", f\"{code_path}/post.py\"],\n",
    "        instance_count=1, \n",
    "        instance_type=\"ml.m5.xlarge\",\n",
    "        volume_size_in_gb=16,\n",
    "        volume_kms_key=None,\n",
    "        output_kms_key=None,\n",
    "        max_runtime_in_seconds=86400,  # default is 24 hours(60*60*24)\n",
    "        sagemaker_session=None,\n",
    "        env=None,\n",
    "        network_config=None\n",
    "    )\n",
    "\n",
    "post_step = ProcessingStep(\n",
    "    \"Post Process\",\n",
    "    processor=train_processor,\n",
    "    job_name=execution_input[\"PostJobName\"],\n",
    "    inputs=post_inputs,\n",
    "    outputs=post_outputs,\n",
    "    container_arguments=[\n",
    "        '--num-of-dataset', str(num_of_segment)\n",
    "    ],\n",
    "    container_entrypoint=[\"python3\", \"/opt/ml/processing/input/code/post.py\"],\n",
    "    wait_for_completion=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 状況通知 Lambda Step の作成\n",
    "\n",
    "ML パイプラインの状況を通知するための Lambda Step を作成します。Lambda 関数自体はすでに作成しているため、LambdaStep でその関数名を指定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stepfunctions.steps.states import Retry\n",
    "\n",
    "status_list = [\n",
    "    \"Training Completed.\",\n",
    "    \"Postprocess Completed.\"\n",
    "]\n",
    "lambda_train_step = stepfunctions.steps.compute.LambdaStep(\n",
    "    \"Train Notification\",\n",
    "    parameters={\n",
    "        \"FunctionName\": lambda_notification_function_name,\n",
    "        \"Payload\": {\n",
    "            \"status\": status_list[0],\n",
    "            \"param.$\": \"$\"\n",
    "        },\n",
    "    },\n",
    ")\n",
    "lambda_train_step.add_retry(\n",
    "    Retry(error_equals=[\"States.TaskFailed\"], interval_seconds=15, max_attempts=2, backoff_rate=4.0)\n",
    ")\n",
    "\n",
    "lambda_post_step = stepfunctions.steps.compute.LambdaStep(\n",
    "    \"Post Notification\",\n",
    "    parameters={\n",
    "        \"FunctionName\": lambda_notification_function_name,\n",
    "        \"Payload\": {\n",
    "            \"status\": status_list[1],\n",
    "            \"param.$\": \"$\"\n",
    "        },\n",
    "    },\n",
    ")\n",
    "lambda_post_step.add_retry(\n",
    "    Retry(error_equals=[\"States.TaskFailed\"], interval_seconds=15, max_attempts=2, backoff_rate=4.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fail State の作成\n",
    "\n",
    "ML パイプラインが失敗したら SNS Topic に通知するために、Catch State と SnsPublishStep を作成します。SnsPublishStep の代わりに、先ほど作成した状況通知 LambdaStep を使用してもかまいません。SNS Topic に送信するメッセージをカスタマイズする必要がなければ SnsPublishStep を使うと便利です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_name = project_name+\"-\" + user_name\n",
    "failed_state = stepfunctions.steps.states.Fail(\n",
    "    \"ML Workflow failed\", cause=\"SageMakerJobFailed\"\n",
    ")\n",
    "\n",
    "sns_step = stepfunctions.steps.service.SnsPublishStep(\n",
    "    \"Error Message\",\n",
    "    comment = \"error\",\n",
    "    parameters={\n",
    "        \"TopicArn\": sns_notification_topic_arn,\n",
    "        \"Message.$\": \"$\",\n",
    "        \"Subject\": f\"Workflow Error: {workflow_name}\"\n",
    "    }\n",
    ")\n",
    "sns_step.next(failed_state)\n",
    "\n",
    "catch_state = stepfunctions.steps.states.Catch(\n",
    "    error_equals=[\"States.ALL\"],\n",
    "    next_step=sns_step,\n",
    ")\n",
    "\n",
    "prep_step.add_catch(catch_state)\n",
    "train_step.add_catch(catch_state)\n",
    "pred_step.add_catch(catch_state)\n",
    "post_step.add_catch(catch_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Functions Workflow の作成\n",
    "\n",
    "作成した各 Step を連結して Workflow を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stepfunctions.workflow import Workflow\n",
    "\n",
    "workflow_graph = Chain([prep_step, train_step, lambda_train_step, pred_step, post_step, lambda_post_step])\n",
    "\n",
    "\n",
    "branching_workflow = Workflow(\n",
    "    name=workflow_name,\n",
    "    definition=workflow_graph,\n",
    "    role=workflow_execution_role,\n",
    ")\n",
    "\n",
    "branching_workflow.create()\n",
    "branching_workflow.update(workflow_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Functions Workflow の動作確認\n",
    "\n",
    "パラメータを指定して Step Functions Workflow を実行します。表示されたリンクから AWS コンソールに移動して今実行した Workflow を確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfn_timestamp = datetime.now(JST).strftime('%Y%m%d-%H%M%S')\n",
    "job_name_prefix = project_name + '-' + user_name\n",
    "data_timestamp =  datetime.now(JST).strftime('%Y%m')\n",
    "prep_job_name = job_name_prefix + '-prep-' + sfn_timestamp\n",
    "train_job_name = job_name_prefix + '-train-' + sfn_timestamp\n",
    "pred_job_name = job_name_prefix + '-pred-' + sfn_timestamp\n",
    "post_job_name = job_name_prefix + '-post-' + sfn_timestamp\n",
    "\n",
    "prep_output_data = f's3://{bucket_name}/{prefix}/prep/{prep_job_name}'\n",
    "train_output_data = f's3://{bucket_name}/{prefix}/train/{train_job_name}'\n",
    "pred_output_data = f's3://{bucket_name}/{prefix}/pred/{pred_job_name}'\n",
    "post_output_data = f's3://{bucket_name}/{prefix}/post/{post_job_name}'\n",
    "\n",
    "pred_args = [\n",
    "        '--num-of-dataset', str(num_of_segment),\n",
    "        '--metrics-threshold', '10000',\n",
    "        '--latest-model-path', train_output_data,\n",
    "        '--previous-model-path', train_output_data\n",
    "    ]\n",
    "\n",
    "execution = branching_workflow.execute(\n",
    "    inputs={\n",
    "        \"TimeStamp\": data_timestamp,\n",
    "        \"PrepJobName\": prep_job_name,\n",
    "        \"PrepInput\": raw_data_s3_path,\n",
    "        \"PrepOutput\": prep_output_data,\n",
    "        \"TrainJobName\": train_job_name,\n",
    "        \"TrainInput\": prep_output_data + '/train',\n",
    "        \"TrainOutput\": train_output_data,\n",
    "        \"PredJobName\": pred_job_name,\n",
    "        \"PredArgs\": pred_args,\n",
    "        \"PredInput\": prep_output_data + '/pred',\n",
    "        \"PredOutput\": pred_output_data,\n",
    "        \"PostJobName\": post_job_name,\n",
    "        \"PostInput\": pred_output_data,\n",
    "        \"PostOutput\": post_output_data,\n",
    "    }\n",
    ")\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(f\"<a href=\\\"https://{region}.console.aws.amazon.com/states/home?region={region}#/executions/details/{execution.execution_arn}\\\" target=\\\"_blank\\\">Step Functions のコンソール</a>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Step Functions Workflow 実行用 Lambda 関数の準備\n",
    "\n",
    "前のセルでは、手動で Step Functions Workflow を実行しましたが、実際は S3 バケットへのファイルアップロードをトリガーに実行したいので、そのための Lambda 関数を作成します。\n",
    "\n",
    "まずは、Lambda 関数で使用する IAM Policy と Role を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_startsfn_function_name = project_name + '-startsfn-' + user_name\n",
    "lambda_startsfn_policy_name = lambda_startsfn_function_name + '-policy'\n",
    "lambda_startsfn_role_name = lambda_startsfn_function_name + '-role'\n",
    "lambda_startsfn_json_name = 'lambda-startsfn-policy.json'\n",
    "\n",
    "assume_role_policy = {\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [{\"Sid\": \"\",\"Effect\": \"Allow\",\"Principal\": {\"Service\":[\"lambda.amazonaws.com\"]},\"Action\": \"sts:AssumeRole\"}]\n",
    "}\n",
    "lambda_startsfn_role_arn = create_policy_role(lambda_startsfn_policy_name, lambda_startsfn_json_name,\n",
    "                   lambda_startsfn_role_name, assume_role_policy)\n",
    "sleep(10) # wait until IAM is created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda 関数で使用するソースコードとライブラリを zip 圧縮します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prepare_lambda_resource(lambda_startsfn_function_name, 'code/start-pipeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step Functions Workflow 実行時に使用するパラメタを環境変数に設定して Lambda 関数を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "envs = {\n",
    "    'SNS_TOPIC_ARN': sns_notification_topic_arn,\n",
    "    'STEPFUNCTION_ARN': branching_workflow.state_machine_arn,\n",
    "    'ECR_PREP_REPO_URI': prep_repository_uri.split(':')[0],\n",
    "    'ECR_TRAIN_REPO_URI': train_repository_uri.split(':')[0],\n",
    "    'BUCKET_NAME': bucket_name,\n",
    "    'SAGEMAKER_ROLE_ARN': role,\n",
    "    'PREFIX': prefix,\n",
    "}\n",
    "lambda_startsfn_function_arn = create_lambda_function(lambda_startsfn_function_name,\n",
    "                                                   lambda_startsfn_function_name,\n",
    "                                                   lambda_startsfn_role_arn,\n",
    "                                                   'index',\n",
    "                                                   envs,\n",
    "                                                   py_version='python3.8')\n",
    "sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 へのファイル作成を AWS Step Functions Workflow 実行用 Lambda 関数のトリガーに設定\n",
    "\n",
    "データアップロード用の S3 バケットの prefix 以下に xxx.run というファイルが作成されたらトリガーを発行して Lambda 関数を実行するように設定します。xxx.run ファイルはデータ準備の入力として使用するファイルと同じ場所に作成される想定です。作成されるファイルの拡張子を `suffix` で指定し、ファイルの作成を監視するパスを `prefix` で指定しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'LambdaFunctionConfigurations': [{\n",
    "        'LambdaFunctionArn': lambda_startsfn_function_arn,\n",
    "        'Events': ['s3:ObjectCreated:*'],\n",
    "        'Filter': {\n",
    "            'Key': {\n",
    "                'FilterRules': [\n",
    "                    {\n",
    "                        'Name': 'suffix',\n",
    "                        'Value': '.run'\n",
    "                    },\n",
    "                    {\n",
    "                        'Name': 'prefix',\n",
    "                        'Value': prefix\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }               \n",
    "    }]\n",
    "}\n",
    "response = lambda_client.add_permission(\n",
    "     FunctionName=lambda_startsfn_function_arn,\n",
    "     StatementId='1',\n",
    "     Action='lambda:InvokeFunction',\n",
    "     Principal='s3.amazonaws.com',\n",
    "     SourceArn=f'arn:aws:s3:::{bucket_name_trigger}',\n",
    "     SourceAccount=account_id\n",
    " )\n",
    "sleep(10)\n",
    "response3 = s3_client.put_bucket_notification_configuration(\n",
    "                            Bucket=bucket_name_trigger,\n",
    "                            NotificationConfiguration=config)\n",
    "sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 動作確認\n",
    "\n",
    "test.run を S3 バケットにアップロードして、Step Functions Workflow が実行されるか確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp test.run s3://$bucket_name_trigger/$prefix/rawdata/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## リソースの削除\n",
    "\n",
    "今回作成したリソースは基本的に利用時のみに料金が発生するものですが、意図しない課金を防ぐために、不要になったらこのノートブックで作成したリソースを削除しましょう。\n",
    "\n",
    "### Step Functions Workflow の削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_list = Workflow.list_workflows()\n",
    "workflow_arn = [d['stateMachineArn'] for d in workflow_list  if d['name']==workflow_name][0]\n",
    "sfn_workflow = Workflow.attach(workflow_arn)\n",
    "try:\n",
    "    sfn_workflow.delete()\n",
    "    print('Delete:', workflow_name)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda 関数の削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_function_list = list(set(lambda_function_list))\n",
    "for f in lambda_function_list:\n",
    "    lambda_client.delete_function(FunctionName=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon ECR リポジトリの削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_image_list = [\n",
    "    train_repository_uri.split('/')[1].split(':')[0],\n",
    "    prep_repository_uri.split('/')[1].split(':')[0]\n",
    "]\n",
    "for i in container_image_list:\n",
    "    try:\n",
    "        ecr_client.delete_repository(\n",
    "            repositoryName=i,\n",
    "            force=True\n",
    "        )\n",
    "        print('Delete:', i)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SNS Topic の削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sns_client.delete_topic(\n",
    "    TopicArn=sns_notification_topic_arn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 バケットの削除\n",
    "\n",
    "S3 バケットを削除したい場合は、以下のセルのコメントアウトを外してから実行してバケットを空にしてください。その後、S3 のコンソールからバケットの削除を実行してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def delete_all_keys_v2(bucket, prefix, dryrun=False):\n",
    "    contents_count = 0\n",
    "    marker = ''\n",
    "\n",
    "    while True:\n",
    "        if marker == '':\n",
    "            response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "        else:\n",
    "            response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix, ContinuationToken=marker)\n",
    "\n",
    "        if 'Contents' in response:\n",
    "            contents = response['Contents']\n",
    "            contents_count = contents_count + len(contents)\n",
    "            for content in contents:\n",
    "                if not dryrun:\n",
    "                    print(\"Deleting: s3://\" + bucket + \"/\" + content['Key'])\n",
    "                    s3_client.delete_object(Bucket=bucket, Key=content['Key'])\n",
    "                else:\n",
    "                    print(\"DryRun: s3://\" + bucket + \"/\" + content['Key'])\n",
    "\n",
    "        if 'NextContinuationToken' in response:\n",
    "            marker = response['NextContinuationToken']\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    print(contents_count, 'file were deleted.')\n",
    "\n",
    "delete_all_keys_v2(bucket_name, '')\n",
    "delete_all_keys_v2(bucket_name_trigger, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IAM Role と Policy の削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_name_list = list(set(role_name_list))\n",
    "policy_arn_list = list(set(policy_arn_list))\n",
    "\n",
    "for r in role_name_list:\n",
    "    try:\n",
    "        detach_role_policies(r)\n",
    "        iam_client.delete_role(RoleName=r)\n",
    "        print('IAM Role 削除完了:', r)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "\n",
    "for p in policy_arn_list:\n",
    "    try:\n",
    "        iam_client.delete_policy(PolicyArn=p)\n",
    "        print('IAM Policy 削除完了:', p)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "# ノートブックインスタンスにアタッチしたポリシーの削除\n",
    "sagemaker_policy_arn = get_policy_arn(sagemaker_policy_name)\n",
    "response = iam_client.detach_role_policy(\n",
    "    RoleName=role.split('/')[2],\n",
    "    PolicyArn=sagemaker_policy_arn\n",
    ")\n",
    "print('\\nこちらの IAM Policy は手動で削除してください。', sagemaker_policy_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Amazon SageMaker Job 単体テスト\n",
    "### Amazon SageMaker Processing でデータ準備\n",
    "\n",
    "このサンプルノートブックでは、Processing Job で任意のセットの学習データを作成して S3 にアップロードします。ここで設定した学習データのセット数に応じて後段の Training の並列数が変わります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import Processor, ProcessingInput, ProcessingOutput\n",
    "\n",
    "num_of_segment = 2\n",
    "\n",
    "prep_timestamp = datetime.now(JST).strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "prep_job_name = project_name + '-' + user_name + '-prep-' + prep_timestamp\n",
    "prep_input_data = raw_data_s3_path\n",
    "prep_output_data = f's3://{bucket_name}/{prefix}/prep/{prep_job_name}'\n",
    "code_path = '/opt/ml/processing/input/code'\n",
    "input_dir = '/opt/ml/processing/input/data'\n",
    "output_dir = '/opt/ml/processing/output'\n",
    "\n",
    "SCRIPT_LOCATION = \"code/prep\"\n",
    "\n",
    "code_s3_path = sagemaker_session.upload_data(\n",
    "    SCRIPT_LOCATION,\n",
    "    bucket=bucket_name,\n",
    "    key_prefix=os.path.join(prefix, SCRIPT_LOCATION, prep_timestamp),\n",
    ")\n",
    "\n",
    "prep_inputs = [\n",
    "    ProcessingInput(\n",
    "        input_name='code',\n",
    "        source=code_s3_path,\n",
    "        destination=code_path),\n",
    "    ProcessingInput(\n",
    "        source=prep_input_data,\n",
    "        destination=input_dir,\n",
    "        input_name=\"data\"\n",
    "    )\n",
    "]\n",
    "\n",
    "prep_outputs = [\n",
    "    ProcessingOutput(\n",
    "        source=output_dir,\n",
    "        destination=prep_output_data,\n",
    "        output_name=\"result\",\n",
    "    )\n",
    "]\n",
    "\n",
    "prep_processor = Processor(\n",
    "        role=role,\n",
    "        image_uri=prep_repository_uri,\n",
    "        entrypoint=[\"python3\", f\"{code_path}/prep.py\"],\n",
    "        instance_count=1, \n",
    "        instance_type=\"ml.m5.xlarge\",\n",
    "        volume_size_in_gb=16,\n",
    "        volume_kms_key=None,\n",
    "        output_kms_key=None,\n",
    "        max_runtime_in_seconds=86400,  # default is 24 hours(60*60*24)\n",
    "        sagemaker_session=None,\n",
    "        env=None,\n",
    "        network_config=None\n",
    "    )\n",
    "\n",
    "prep_processor.run(\n",
    "    job_name=prep_job_name,\n",
    "    inputs=prep_inputs,\n",
    "     outputs=prep_outputs,\n",
    "    arguments=['--num-of-dataset', str(num_of_segment)],\n",
    "    logs=False,\n",
    "    wait=False\n",
    ")\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(f\"<a href=\\\"https://s3.console.aws.amazon.com/s3/buckets/{bucket_name}?region={region}&prefix={prefix}/prep/{prep_job_name}/&showversions=false\\\" target=\\\"_blank\\\">準備したデータ (S3)</a>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs1 = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix+'/1')\n",
    "# inputs2 = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix+'/2')\n",
    "# print(inputs1)\n",
    "# print(inputs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon SageMaker Processing で AutoGluon モデルの並列学習\n",
    "\n",
    "テータ準備の Processing Job が完了したら、準備されたデータを使ってモデルを学習します。テータ準備の Processing Job の出力データを入力として、`s3_data_distribution_type='ShardedByS3Key'` としてデータ並列（今回は1データ1インスタンス）でモデルの学習を実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import Processor, ProcessingInput, ProcessingOutput\n",
    "\n",
    "train_timestamp = datetime.now(JST).strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "train_job_name = project_name + '-' + user_name + '-train-' + train_timestamp\n",
    "train_input_data = prep_output_data + '/train'\n",
    "train_output_data = f's3://{bucket_name}/{prefix}/train/{train_job_name}'\n",
    "code_path = '/opt/ml/processing/input/code'\n",
    "input_dir = '/opt/ml/processing/input/data'\n",
    "output_dir = '/opt/ml/processing/output'\n",
    "\n",
    "SCRIPT_LOCATION = \"code/train\"\n",
    "\n",
    "code_s3_path = sagemaker_session.upload_data(\n",
    "    SCRIPT_LOCATION,\n",
    "    bucket=bucket_name,\n",
    "    key_prefix=os.path.join(prefix, SCRIPT_LOCATION, train_timestamp),\n",
    ")\n",
    "\n",
    "train_inputs = [\n",
    "    ProcessingInput(\n",
    "        input_name='code',\n",
    "        source=code_s3_path,\n",
    "        destination=code_path),\n",
    "    ProcessingInput(\n",
    "        source=train_input_data,\n",
    "        destination=input_dir,\n",
    "        input_name=\"data\",\n",
    "        s3_data_distribution_type='ShardedByS3Key'\n",
    "    )\n",
    "]\n",
    "\n",
    "train_outputs = [\n",
    "    ProcessingOutput(\n",
    "        source=output_dir,\n",
    "        destination=train_output_data,\n",
    "        output_name=\"result\",\n",
    "    )\n",
    "]\n",
    "\n",
    "train_processor = Processor(\n",
    "        role=role,\n",
    "        image_uri=train_repository_uri,\n",
    "        entrypoint=[\"python3\", f\"{code_path}/train.py\"],\n",
    "        instance_count=num_of_segment, \n",
    "        instance_type=\"ml.m5.xlarge\",\n",
    "        volume_size_in_gb=16,\n",
    "        volume_kms_key=None,\n",
    "        output_kms_key=None,\n",
    "        max_runtime_in_seconds=86400,  # default is 24 hours(60*60*24)\n",
    "        sagemaker_session=None,\n",
    "        env=None,\n",
    "        network_config=None\n",
    "    )\n",
    "\n",
    "train_processor.run(\n",
    "    job_name=train_job_name,\n",
    "    inputs=train_inputs,\n",
    "     outputs=train_outputs,\n",
    "    arguments=['--num-of-dataset', str(num_of_segment)],\n",
    "    logs=False,\n",
    "    wait=False\n",
    ")\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(f\"<a href=\\\"https://s3.console.aws.amazon.com/s3/buckets/{bucket_name}?region={region}&prefix={prefix}/train/{train_job_name}/&showversions=false\\\" target=\\\"_blank\\\">学習済みモデル (S3)</a>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon SageMaker Processing でバッチ推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import Processor, ProcessingInput, ProcessingOutput\n",
    "\n",
    "\n",
    "pred_timestamp = datetime.now(JST).strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "pred_job_name = project_name + '-' + user_name + '-pred-' + pred_timestamp\n",
    "pred_input_data = prep_output_data + '/pred'\n",
    "pred_output_data = f's3://{bucket_name}/{prefix}/pred/{pred_job_name}'\n",
    "code_path = '/opt/ml/processing/input/code'\n",
    "input_dir = '/opt/ml/processing/input/data'\n",
    "output_dir = '/opt/ml/processing/output'\n",
    "\n",
    "SCRIPT_LOCATION = \"code/pred\"\n",
    "\n",
    "code_s3_path = sagemaker_session.upload_data(\n",
    "    SCRIPT_LOCATION,\n",
    "    bucket=bucket_name,\n",
    "    key_prefix=os.path.join(prefix, SCRIPT_LOCATION, pred_timestamp),\n",
    ")\n",
    "\n",
    "pred_inputs = [\n",
    "    ProcessingInput(\n",
    "        input_name='code',\n",
    "        source=code_s3_path,\n",
    "        destination=code_path),\n",
    "    ProcessingInput(\n",
    "        source=pred_input_data,\n",
    "        destination=input_dir,\n",
    "        input_name=\"data\",\n",
    "        s3_data_distribution_type='ShardedByS3Key'\n",
    "    )\n",
    "]\n",
    "\n",
    "pred_outputs = [\n",
    "    ProcessingOutput(\n",
    "        source=output_dir,\n",
    "        destination=pred_output_data,\n",
    "        output_name=\"result\",\n",
    "    )\n",
    "]\n",
    "\n",
    "pred_processor = Processor(\n",
    "        role=role,\n",
    "        image_uri=train_repository_uri,\n",
    "        entrypoint=[\"python3\", f\"{code_path}/pred.py\"],\n",
    "        instance_count=num_of_segment, \n",
    "        instance_type=\"ml.m5.xlarge\",\n",
    "        volume_size_in_gb=16,\n",
    "        volume_kms_key=None,\n",
    "        output_kms_key=None,\n",
    "        max_runtime_in_seconds=86400,  # default is 24 hours(60*60*24)\n",
    "        sagemaker_session=None,\n",
    "        env=None,\n",
    "        network_config=None\n",
    "    )\n",
    "\n",
    "pred_processor.run(\n",
    "    job_name=pred_job_name,\n",
    "    inputs=pred_inputs,\n",
    "     outputs=pred_outputs,\n",
    "    arguments=[\n",
    "        '--num-of-dataset', str(num_of_segment),\n",
    "        '--metrics-threshold', '10000',\n",
    "        '--latest-model-path', train_output_data,\n",
    "        '--previous-model-path', train_output_data\n",
    "              ],\n",
    "    logs=False,\n",
    "    wait=False\n",
    ")\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(f\"<a href=\\\"https://s3.console.aws.amazon.com/s3/buckets/{bucket_name}?region={region}&prefix={prefix}/pred/{pred_job_name}/&showversions=false\\\" target=\\\"_blank\\\">バッチ推論結果 (S3)</a>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon SageMaker Processing で後処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import Processor, ProcessingInput, ProcessingOutput\n",
    "\n",
    "post_timestamp = datetime.now(JST).strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "post_job_name = project_name + '-' + user_name + '-post-' + post_timestamp\n",
    "post_input_data = pred_output_data\n",
    "post_output_data = f's3://{bucket_name}/{prefix}/post/{post_job_name}'\n",
    "code_path = '/opt/ml/processing/input/code'\n",
    "input_dir = '/opt/ml/processing/input/data'\n",
    "output_dir = '/opt/ml/processing/output'\n",
    "\n",
    "SCRIPT_LOCATION = \"code/post\"\n",
    "\n",
    "code_s3_path = sagemaker_session.upload_data(\n",
    "    SCRIPT_LOCATION,\n",
    "    bucket=bucket_name,\n",
    "    key_prefix=os.path.join(prefix, SCRIPT_LOCATION, post_timestamp),\n",
    ")\n",
    "\n",
    "post_inputs = [\n",
    "    ProcessingInput(\n",
    "        input_name='code',\n",
    "        source=code_s3_path,\n",
    "        destination=code_path),\n",
    "    ProcessingInput(\n",
    "        source=post_input_data,\n",
    "        destination=input_dir,\n",
    "        input_name=\"data\"\n",
    "    )\n",
    "]\n",
    "\n",
    "post_outputs = [\n",
    "    ProcessingOutput(\n",
    "        source=output_dir,\n",
    "        destination=post_output_data,\n",
    "        output_name=\"result\",\n",
    "    )\n",
    "]\n",
    "\n",
    "post_processor = Processor(\n",
    "        role=role,\n",
    "        image_uri=prep_repository_uri,\n",
    "        entrypoint=[\"python3\", f\"{code_path}/post.py\"],\n",
    "        instance_count=1, \n",
    "#         instance_type=\"ml.m5.xlarge\",\n",
    "        instance_type=\"local\",\n",
    "        volume_size_in_gb=16,\n",
    "        volume_kms_key=None,\n",
    "        output_kms_key=None,\n",
    "        max_runtime_in_seconds=86400,  # default is 24 hours(60*60*24)\n",
    "        sagemaker_session=None,\n",
    "        env=None,\n",
    "        network_config=None\n",
    "    )\n",
    "\n",
    "post_processor.run(\n",
    "    job_name=post_job_name,\n",
    "    inputs=post_inputs,\n",
    "     outputs=post_outputs,\n",
    "    arguments=['--num-of-dataset', str(num_of_segment)],\n",
    "    logs=False,\n",
    "    wait=False\n",
    ")\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(f\"<a href=\\\"https://s3.console.aws.amazon.com/s3/buckets/{bucket_name}?region={region}&prefix={prefix}/post/{post_job_name}/&showversions=false\\\" target=\\\"_blank\\\">後処理データ (S3)</a>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "255.382px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
